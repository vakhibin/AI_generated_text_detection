# @package _global_.training_params
num_epochs: 1
lr: 0.001
weight_decay: 0.0
optimizer: "adam" # "adam", "adamw", "sgd"
scheduler: null # "reduce_lr_on_plateau", "cosine", "step", null
scheduler_patience: 5
scheduler_factor: 0.1
log_interval: 10
early_stopping_patience: 5
