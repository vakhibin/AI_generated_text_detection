"""Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Lightning Ð¼Ð¾Ð´ÑƒÐ»ÑŒ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐ° Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹."""

import torch
import lightning as L
import torch.nn as nn
import torch.nn.functional as F
from torchmetrics import MetricCollection
import torchmetrics
from typing import Optional, Dict, Any

from ai_generated_text_detection.logger import logger


class UniversalModelModule(L.LightningModule):
    """
    Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Lightning Ð¼Ð¾Ð´ÑƒÐ»ÑŒ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐ° Ð»ÑŽÐ±Ñ‹Ñ… PyTorch Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹.

    Parameters
    ----------
    model : nn.Module
        PyTorch Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ.
    model_type : str
        Ð¢Ð¸Ð¿ Ð¼Ð¾Ð´ÐµÐ»Ð¸ ("lstm", "transformer", "baseline").
    learning_rate : float
        Learning rate Ð´Ð»Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°.
    weight_decay : float, optional
        Weight decay Ð´Ð»Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð° (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ 0.0).
    optimizer : str, optional
        Ð¢Ð¸Ð¿ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð° ("adam", "adamw", "sgd") (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ "adam").
    scheduler : str, optional
        Ð¢Ð¸Ð¿ scheduler ("reduce_lr_on_plateau", "cosine", "step", None) (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ None).
    scheduler_patience : int, optional
        Patience Ð´Ð»Ñ ReduceLROnPlateau (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ 5).
    scheduler_factor : float, optional
        Factor Ð´Ð»Ñ ReduceLROnPlateau (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ 0.1).
    log_interval : int, optional
        Ð˜Ð½Ñ‚ÐµÑ€Ð²Ð°Ð» Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼ÐµÑ‚Ñ€Ð¸Ðº (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ 10).
    """

    def __init__(
        self,
        model: nn.Module,
        model_type: str = "custom",
        learning_rate: float = 1e-3,
        weight_decay: float = 0.0,
        optimizer: str = "adam",
        scheduler: Optional[str] = None,
        scheduler_patience: int = 5,
        scheduler_factor: float = 0.1,
        log_interval: int = 10,
    ):
        super().__init__()

        self.model = model
        self.model_type = model_type.lower()
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.optimizer_type = optimizer.lower()
        self.scheduler_type = scheduler.lower() if scheduler else None
        self.scheduler_patience = scheduler_patience
        self.scheduler_factor = scheduler_factor
        self.log_interval = log_interval

        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð³Ð¸Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð´Ð»Ñ Ð²Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸
        self.save_hyperparameters(ignore=["model"])

        # Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ Ð´Ð»Ñ Ð±Ð¸Ð½Ð°Ñ€Ð½Ð¾Ð¹ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸
        self.loss_fn = nn.BCEWithLogitsLoss()

        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼ÐµÑ‚Ñ€Ð¸Ðº
        self._init_metrics()

    def _init_metrics(self) -> None:
        """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð´Ð»Ñ Ñ‚Ñ€ÐµÐ¹Ð½Ð° Ð¸ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸."""

        # ÐžÐ±Ñ‰Ð¸Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð´Ð»Ñ Ð±Ð¸Ð½Ð°Ñ€Ð½Ð¾Ð¹ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸
        common_metrics = {
            "accuracy": torchmetrics.Accuracy(task="binary"),
            "precision": torchmetrics.Precision(task="binary"),
            "recall": torchmetrics.Recall(task="binary"),
            "f1": torchmetrics.F1Score(task="binary"),
            "auc": torchmetrics.AUROC(task="binary"),
        }

        # ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð´Ð»Ñ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ¸
        self.train_metrics = MetricCollection({**common_metrics}, prefix="train_")

        # ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð´Ð»Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸
        self.val_metrics = MetricCollection({**common_metrics}, prefix="val_")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        ÐŸÑ€ÑÐ¼Ð¾Ð¹ Ð¿Ñ€Ð¾Ñ…Ð¾Ð´ Ñ‡ÐµÑ€ÐµÐ· Ð¼Ð¾Ð´ÐµÐ»ÑŒ.

        Parameters
        ----------
        x : torch.Tensor
            Ð’Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ.

        Returns
        -------
        torch.Tensor
            Ð’Ñ‹Ñ…Ð¾Ð´ Ð¼Ð¾Ð´ÐµÐ»Ð¸.
        """
        return self.model(x)

    def _common_step(self, batch: tuple, stage: str) -> torch.Tensor:
        """
        ÐžÐ±Ñ‰Ð¸Ð¹ ÑˆÐ°Ð³ Ð´Ð»Ñ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ¸ Ð¸ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸.

        Parameters
        ----------
        batch : tuple
            Ð‘Ð°Ñ‚Ñ‡ Ð´Ð°Ð½Ð½Ñ‹Ñ… (x, y).
        stage : str
            Ð¡Ñ‚Ð°Ð´Ð¸Ñ ("train", "val").

        Returns
        -------
        torch.Tensor
            Loss.
        """
        x, y = batch
        y_hat = self.model(x)
        loss = self.loss_fn(y_hat, y)

        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð¸ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸
        probs = torch.sigmoid(y_hat)
        preds = (probs > 0.5).float()

        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
        if stage == "train":
            self.train_metrics(preds, y, probs=probs)
        else:  # val
            self.val_metrics(preds, y, probs=probs)

        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ loss
        self.log(f"{stage}_loss", loss, prog_bar=True, logger=True)

        return loss

    def training_step(self, batch: tuple, batch_idx: int) -> torch.Tensor:
        """
        Ð¨Ð°Ð³ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ¸.

        Parameters
        ----------
        batch : tuple
            Ð‘Ð°Ñ‚Ñ‡ Ð´Ð°Ð½Ð½Ñ‹Ñ… (x, y).
        batch_idx : int
            Ð˜Ð½Ð´ÐµÐºÑ Ð±Ð°Ñ‚Ñ‡Ð°.

        Returns
        -------
        torch.Tensor
            Loss.
        """
        loss = self._common_step(batch, "train")

        # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ LSTM
        if self.model_type == "lstm" and (
            self.trainer.current_epoch % self.log_interval == 0
            or self.trainer.current_epoch == self.trainer.max_epochs - 1
        ):
            self.log(
                f"Epoch {self.trainer.current_epoch}, Batch {batch_idx}: Train loss = {loss:.4f}",
                logger=True,
            )

        return loss

    def validation_step(self, batch: tuple, batch_idx: int) -> torch.Tensor:
        """
        Ð¨Ð°Ð³ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸.

        Parameters
        ----------
        batch : tuple
            Ð‘Ð°Ñ‚Ñ‡ Ð´Ð°Ð½Ð½Ñ‹Ñ… (x, y).
        batch_idx : int
            Ð˜Ð½Ð´ÐµÐºÑ Ð±Ð°Ñ‚Ñ‡Ð°.

        Returns
        -------
        torch.Tensor
            Loss.
        """
        loss = self._common_step(batch, "val")

        # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ LSTM
        if self.model_type == "lstm" and (
            self.trainer.current_epoch % self.log_interval == 0
            or self.trainer.current_epoch == self.trainer.max_epochs - 1
        ):
            preds = (torch.sigmoid(self(batch[0])) > 0.5).float()
            acc = (preds == batch[1]).float().mean()
            self.log(
                f"Epoch {self.trainer.current_epoch}, Batch {batch_idx}: Val acc = {acc:.4f}",
                logger=True,
            )

        return loss

    def on_train_epoch_end(self) -> None:
        """
        Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ÑÑ Ð² ÐºÐ¾Ð½Ñ†Ðµ ÑÐ¿Ð¾Ñ…Ð¸ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ¸.
        Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÑ‚ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð¸ ÑÐ±Ñ€Ð°ÑÑ‹Ð²Ð°ÐµÑ‚ Ð¸Ñ….
        """
        if self.current_epoch % self.log_interval == 0:
            metrics = self.train_metrics.compute()
            self._log_metrics(metrics, "train")

        self.train_metrics.reset()

    def on_validation_epoch_end(self) -> None:
        """
        Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ÑÑ Ð² ÐºÐ¾Ð½Ñ†Ðµ ÑÐ¿Ð¾Ñ…Ð¸ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸.
        Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÑ‚ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð¸ ÑÐ±Ñ€Ð°ÑÑ‹Ð²Ð°ÐµÑ‚ Ð¸Ñ….
        """
        if self.current_epoch % self.log_interval == 0:
            metrics = self.val_metrics.compute()
            self._log_metrics(metrics, "val")

        self.val_metrics.reset()

    def _log_metrics(self, metrics: Dict[str, torch.Tensor], stage: str) -> None:
        """
        Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÑ‚ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸.

        Parameters
        ----------
        metrics : Dict[str, torch.Tensor]
            Ð¡Ð»Ð¾Ð²Ð°Ñ€ÑŒ Ñ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼Ð¸.
        stage : str
            Ð¡Ñ‚Ð°Ð´Ð¸Ñ ("train", "val").
        """
        logger.info(f"\nðŸ“Š Ð­Ð¿Ð¾Ñ…Ð° {self.current_epoch + 1} - {stage.title()}:")
        for name, value in metrics.items():
            clean_name = name.replace(f"{stage}_", "").title()
            logger.info(f"  {clean_name}: {value:.4f}")
            self.log(name, value, logger=True)

    def configure_optimizers(self) -> Dict[str, Any]:
        """
        ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ð¸ scheduler.

        Returns
        -------
        Dict[str, Any]
            ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð° Ð¸ scheduler.
        """
        # Ð’Ñ‹Ð±Ð¾Ñ€ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°
        if self.optimizer_type == "adam":
            optimizer = torch.optim.Adam(
                self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay
            )
        elif self.optimizer_type == "adamw":
            optimizer = torch.optim.AdamW(
                self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay
            )
        elif self.optimizer_type == "sgd":
            optimizer = torch.optim.SGD(
                self.parameters(),
                lr=self.learning_rate,
                weight_decay=self.weight_decay,
                momentum=0.9,
            )
        else:
            raise ValueError(f"ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ð¹ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€: {self.optimizer_type}")

        # ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ scheduler
        if self.scheduler_type == "reduce_lr_on_plateau":
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode="min",
                patience=self.scheduler_patience,
                factor=self.scheduler_factor,
                verbose=True,
            )
            return {
                "optimizer": optimizer,
                "lr_scheduler": {
                    "scheduler": scheduler,
                    "monitor": "val_loss",
                    "interval": "epoch",
                    "frequency": 1,
                },
            }
        elif self.scheduler_type == "cosine":
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=self.trainer.max_epochs,
                eta_min=self.learning_rate * 0.01,
            )
            return [optimizer], [scheduler]
        elif self.scheduler_type == "step":
            scheduler = torch.optim.lr_scheduler.StepLR(
                optimizer, step_size=self.trainer.max_epochs // 3, gamma=0.1
            )
            return [optimizer], [scheduler]
        else:
            # Ð‘ÐµÐ· scheduler
            return optimizer

    def predict_step(
        self, batch: tuple, batch_idx: int, dataloader_idx: int = 0
    ) -> Dict[str, torch.Tensor]:
        """
        Ð¨Ð°Ð³ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð´Ð»Ñ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐ°.

        Parameters
        ----------
        batch : tuple
            Ð‘Ð°Ñ‚Ñ‡ Ð´Ð°Ð½Ð½Ñ‹Ñ… (x, y).
        batch_idx : int
            Ð˜Ð½Ð´ÐµÐºÑ Ð±Ð°Ñ‚Ñ‡Ð°.
        dataloader_idx : int
            Ð˜Ð½Ð´ÐµÐºÑ Ð´Ð°Ñ‚Ð°Ð»Ð¾Ð°Ð´ÐµÑ€Ð°.

        Returns
        -------
        Dict[str, torch.Tensor]
            Ð¡Ð»Ð¾Ð²Ð°Ñ€ÑŒ Ñ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸ÑÐ¼Ð¸, Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑÐ¼Ð¸ Ð¸ Ð¸ÑÑ‚Ð¸Ð½Ð½Ñ‹Ð¼Ð¸ Ð¼ÐµÑ‚ÐºÐ°Ð¼Ð¸.
        """
        x, y = batch
        with torch.no_grad():
            y_hat = self.model(x)
            probs = torch.sigmoid(y_hat)
            preds = (probs > 0.5).float()

        return {"predictions": preds, "probabilities": probs, "labels": y}
